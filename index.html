<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>test</title>
    <!-- Bootstrap CSS -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
	<style type="text/css">
		  table {
			width: 100%;
			table-layout: fixed;
		  }

		  audio {
			width: 100%;
		  }

		  thead>tr>th:first-child {
			width: 96px;
		  }
		td{
			text-align: center;
		}
			
		  @media (max-width: 767px) {
			.big-screen {
			  display: none;
			}
		  }

		  @media (min-width: 767px) {
			.small-screen {
			  display: none;
			}
		  }
		.center {
		  display: block;
		  margin-left: auto;
		  margin-right: auto;
		}

		
	</style>
</head>
<body>
    
	<div class="jumbotron bg-secondary text-center">
	  <div class="container">
		<div class="row align-items-center">
		  <div class="col-md-12">
			 <h1><a class="text-light"> Mixture of Emotion Dependent Experts: Facial Expressions Recognition in Videos through Stacked Expert Models</a> </h1>
		  </div>
		</div>
	  </div>
	</div>
	
	<div class="container">
 		<img src="./moede.png"  class="center" width="100%" />
		<p class="lead">Recent advancements in <em>dynamic facial expression recognition</em> (DFER) have predominantly utilized static features, 
			which are theoretically inferior to dynamic features. However, models fully trained with dynamic features often suffer from overfitting 
			due to the limited size and diversity of the training data. A significant challenge with existing models based on static features in recognizing 
			emotions from videos is their tendency to form biased representations, often unbalanced or skewed towards more prevalent or basic emotional features 
			present in the static domain, especially with posed expression. Therefore, this approach under-represents the nuances present in the dynamic domain. 
			To address this issue, our study introduces a novel approach that we refer to as <em>mixture of emotion-dependent experts</em> (MoEDE). This strategy relies 
			on emotion-specific feature extractors to produce more diverse emotional static features to train DFER systems. Each emotion-dependent expert focuses 
			exclusively on one emotional category, formulating the problem as binary classifiers. Our DFER model combines these static representations with recurrent 
			models, modeling their temporal relationships. The proposed MoEDE DFER approach achieves a macro F1-score of 74.5%, marking a significant improvement 
			over the baseline, which presented a macro F1-score of 70.9% using a single model trained on all classes in the AffectNet dataset. Additionally, our 
			proposed approach shows consistent improvements compared to the four popular baselines. 
		</p>
    </div>

    <!-- Bootstrap JS and dependencies -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
